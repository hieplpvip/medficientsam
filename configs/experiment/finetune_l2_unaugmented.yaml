# @package _global_

# to execute this experiment run:
# python train.py experiment=finetune_l2_unaugmented

defaults:
  - override /callbacks: finetune
  - override /data: finetune_medsam
  - override /model: finetune

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ['finetune', 'efficientvit']

seed: 42

trainer:
  min_epochs: 8
  max_epochs: 8
  gradient_clip_val: 0.5

data:
  dataset:
    image_encoder_input_size: 512
    prompt_encoder_input_size: 1024
    scale_image: true
    normalize_image: false
    aug_transform: null

  pin_memory: true

model:
  model:
    _target_: src.models.base_sam.BaseSAM.construct_from

    original_sam:
      _target_: src.models.segment_anything.build_sam_vit_b
      checkpoint: ${paths.weights_dir}/medsam/medsam_vit_b.pth

    distill_lit_module:
      _target_: src.models.distill_module.DistillLitModule.load_from_checkpoint
      checkpoint_path: ${paths.weights_dir}/distilled-l2/step_400000.ckpt
      student_net:
        _target_: src.models.efficientvit.sam_model_zoo.create_sam_model
        name: l2
        pretrained: false
      teacher_net:
        _target_: src.models.segment_anything.build_sam_vit_b
        checkpoint: ${paths.weights_dir}/medsam/medsam_vit_b.pth

logger:
  wandb:
    tags: ${tags}
    group: 'medficientsam'
    name: 'finetune_efficientvit'
