{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from torchinfo import summary\n",
    "from src.models.base_sam import BaseSAM\n",
    "from src.models.efficientvit.sam_model_zoo import create_sam_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = torch.randn(1, 4)\n",
    "\n",
    "class FlopCountWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.model(image=image, boxes=boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_model = create_sam_model(name=\"l0\", pretrained=False).to(\"cpu\")\n",
    "l0_sam = BaseSAM.construct_from(original_sam=l0_model)\n",
    "\n",
    "image = torch.randn(1, 3, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0 params: 34.79 M\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"L0 params:\",\n",
    "    round(\n",
    "        summary(l0_sam, image=image, boxes=boxes, device=\"cpu\").total_params / 1000000,\n",
    "        2,\n",
    "    ),\n",
    "    \"M\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 38 time(s)\n",
      "Unsupported operator aten::gelu encountered 32 time(s)\n",
      "Unsupported operator aten::add encountered 50 time(s)\n",
      "Unsupported operator aten::mul encountered 18 time(s)\n",
      "Unsupported operator aten::pad encountered 4 time(s)\n",
      "Unsupported operator aten::div encountered 17 time(s)\n",
      "Unsupported operator aten::upsample_bicubic2d encountered 3 time(s)\n",
      "Unsupported operator aten::mean encountered 4 time(s)\n",
      "Unsupported operator aten::sub encountered 7 time(s)\n",
      "Unsupported operator aten::square encountered 1 time(s)\n",
      "Unsupported operator aten::sqrt encountered 2 time(s)\n",
      "Unsupported operator aten::clone encountered 1 time(s)\n",
      "Unsupported operator aten::sin encountered 2 time(s)\n",
      "Unsupported operator aten::cos encountered 2 time(s)\n",
      "Unsupported operator aten::cumsum encountered 2 time(s)\n",
      "Unsupported operator aten::repeat_interleave encountered 2 time(s)\n",
      "Unsupported operator aten::softmax encountered 7 time(s)\n",
      "Unsupported operator aten::pow encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "model.image_encoder.backbone.stages.0.op_list.1.shortcut, model.image_encoder.backbone.stages.1.op_list.1.shortcut, model.image_encoder.backbone.stages.2.op_list.1.shortcut, model.image_encoder.backbone.stages.3.op_list.1.shortcut, model.image_encoder.backbone.stages.3.op_list.2.shortcut, model.image_encoder.backbone.stages.3.op_list.3.shortcut, model.image_encoder.backbone.stages.3.op_list.4.shortcut, model.image_encoder.backbone.stages.4.op_list.1.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.1.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.2.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.2.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.3.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.3.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.4.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.4.local_module.shortcut, model.image_encoder.neck.middle.op_list.0.shortcut, model.image_encoder.neck.middle.op_list.1.shortcut, model.image_encoder.neck.middle.op_list.2.shortcut, model.image_encoder.neck.middle.op_list.3.shortcut, model.mask_decoder.iou_token, model.mask_decoder.mask_tokens, model.prompt_encoder.mask_downscaling, model.prompt_encoder.mask_downscaling.0, model.prompt_encoder.mask_downscaling.1, model.prompt_encoder.mask_downscaling.2, model.prompt_encoder.mask_downscaling.3, model.prompt_encoder.mask_downscaling.4, model.prompt_encoder.mask_downscaling.5, model.prompt_encoder.mask_downscaling.6, model.prompt_encoder.no_mask_embed, model.prompt_encoder.not_a_point_embed, model.prompt_encoder.point_embeddings.0, model.prompt_encoder.point_embeddings.1, model.prompt_encoder.point_embeddings.2, model.prompt_encoder.point_embeddings.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0 flops: 36.8 G\n"
     ]
    }
   ],
   "source": [
    "flops = FlopCountAnalysis(FlopCountWrapper(l0_sam), image)\n",
    "print(\"L0 flops:\", round(flops.total() / 1e9, 2), \"G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_model = create_sam_model(name=\"l1\", pretrained=False).to(\"cpu\")\n",
    "l1_sam = BaseSAM.construct_from(original_sam=l1_model)\n",
    "\n",
    "image = torch.randn(1, 3, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 image encoder params: 43.59 M\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"L1 image encoder params:\",\n",
    "    round(\n",
    "        summary(l1_model.image_encoder, (1, 3, 512, 512), device=\"cpu\").total_params\n",
    "        / 1000000,\n",
    "        2,\n",
    "    ),\n",
    "    \"M\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 50 time(s)\n",
      "Unsupported operator aten::gelu encountered 42 time(s)\n",
      "Unsupported operator aten::add encountered 39 time(s)\n",
      "Unsupported operator aten::mul encountered 7 time(s)\n",
      "Unsupported operator aten::pad encountered 6 time(s)\n",
      "Unsupported operator aten::div encountered 7 time(s)\n",
      "Unsupported operator aten::upsample_bicubic2d encountered 3 time(s)\n",
      "Unsupported operator aten::mean encountered 2 time(s)\n",
      "Unsupported operator aten::sub encountered 1 time(s)\n",
      "Unsupported operator aten::square encountered 1 time(s)\n",
      "Unsupported operator aten::sqrt encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "backbone.stages.0.op_list.1.shortcut, backbone.stages.1.op_list.1.shortcut, backbone.stages.2.op_list.1.shortcut, backbone.stages.3.op_list.1.shortcut, backbone.stages.3.op_list.2.shortcut, backbone.stages.3.op_list.3.shortcut, backbone.stages.3.op_list.4.shortcut, backbone.stages.3.op_list.5.shortcut, backbone.stages.3.op_list.6.shortcut, backbone.stages.4.op_list.1.context_module.shortcut, backbone.stages.4.op_list.1.local_module.shortcut, backbone.stages.4.op_list.2.context_module.shortcut, backbone.stages.4.op_list.2.local_module.shortcut, backbone.stages.4.op_list.3.context_module.shortcut, backbone.stages.4.op_list.3.local_module.shortcut, backbone.stages.4.op_list.4.context_module.shortcut, backbone.stages.4.op_list.4.local_module.shortcut, backbone.stages.4.op_list.5.context_module.shortcut, backbone.stages.4.op_list.5.local_module.shortcut, backbone.stages.4.op_list.6.context_module.shortcut, backbone.stages.4.op_list.6.local_module.shortcut, neck.middle.op_list.0.shortcut, neck.middle.op_list.1.shortcut, neck.middle.op_list.2.shortcut, neck.middle.op_list.3.shortcut, neck.middle.op_list.4.shortcut, neck.middle.op_list.5.shortcut, neck.middle.op_list.6.shortcut, neck.middle.op_list.7.shortcut\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 image encoder flops: 49.23 G\n"
     ]
    }
   ],
   "source": [
    "flops = FlopCountAnalysis(l1_model.image_encoder, image)\n",
    "print(\"L1 image encoder flops:\", round(flops.total() / 1e9, 2), \"G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 params: 47.65 M\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"L1 params:\",\n",
    "    round(\n",
    "        summary(l1_sam, image=image, boxes=boxes, device=\"cpu\").total_params / 1000000,\n",
    "        2,\n",
    "    ),\n",
    "    \"M\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 52 time(s)\n",
      "Unsupported operator aten::gelu encountered 44 time(s)\n",
      "Unsupported operator aten::add encountered 62 time(s)\n",
      "Unsupported operator aten::mul encountered 20 time(s)\n",
      "Unsupported operator aten::pad encountered 6 time(s)\n",
      "Unsupported operator aten::div encountered 19 time(s)\n",
      "Unsupported operator aten::upsample_bicubic2d encountered 3 time(s)\n",
      "Unsupported operator aten::mean encountered 4 time(s)\n",
      "Unsupported operator aten::sub encountered 7 time(s)\n",
      "Unsupported operator aten::square encountered 1 time(s)\n",
      "Unsupported operator aten::sqrt encountered 2 time(s)\n",
      "Unsupported operator aten::clone encountered 1 time(s)\n",
      "Unsupported operator aten::sin encountered 2 time(s)\n",
      "Unsupported operator aten::cos encountered 2 time(s)\n",
      "Unsupported operator aten::cumsum encountered 2 time(s)\n",
      "Unsupported operator aten::repeat_interleave encountered 2 time(s)\n",
      "Unsupported operator aten::softmax encountered 7 time(s)\n",
      "Unsupported operator aten::pow encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "model.image_encoder.backbone.stages.0.op_list.1.shortcut, model.image_encoder.backbone.stages.1.op_list.1.shortcut, model.image_encoder.backbone.stages.2.op_list.1.shortcut, model.image_encoder.backbone.stages.3.op_list.1.shortcut, model.image_encoder.backbone.stages.3.op_list.2.shortcut, model.image_encoder.backbone.stages.3.op_list.3.shortcut, model.image_encoder.backbone.stages.3.op_list.4.shortcut, model.image_encoder.backbone.stages.3.op_list.5.shortcut, model.image_encoder.backbone.stages.3.op_list.6.shortcut, model.image_encoder.backbone.stages.4.op_list.1.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.1.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.2.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.2.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.3.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.3.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.4.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.4.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.5.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.5.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.6.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.6.local_module.shortcut, model.image_encoder.neck.middle.op_list.0.shortcut, model.image_encoder.neck.middle.op_list.1.shortcut, model.image_encoder.neck.middle.op_list.2.shortcut, model.image_encoder.neck.middle.op_list.3.shortcut, model.image_encoder.neck.middle.op_list.4.shortcut, model.image_encoder.neck.middle.op_list.5.shortcut, model.image_encoder.neck.middle.op_list.6.shortcut, model.image_encoder.neck.middle.op_list.7.shortcut, model.mask_decoder.iou_token, model.mask_decoder.mask_tokens, model.prompt_encoder.mask_downscaling, model.prompt_encoder.mask_downscaling.0, model.prompt_encoder.mask_downscaling.1, model.prompt_encoder.mask_downscaling.2, model.prompt_encoder.mask_downscaling.3, model.prompt_encoder.mask_downscaling.4, model.prompt_encoder.mask_downscaling.5, model.prompt_encoder.mask_downscaling.6, model.prompt_encoder.no_mask_embed, model.prompt_encoder.not_a_point_embed, model.prompt_encoder.point_embeddings.0, model.prompt_encoder.point_embeddings.1, model.prompt_encoder.point_embeddings.2, model.prompt_encoder.point_embeddings.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 flops: 51.05 G\n"
     ]
    }
   ],
   "source": [
    "flops = FlopCountAnalysis(FlopCountWrapper(l1_sam), image)\n",
    "print(\"L1 flops:\", round(flops.total() / 1e9, 2), \"G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model = create_sam_model(name=\"l2\", pretrained=False)\n",
    "l2_sam = BaseSAM.construct_from(original_sam=l2_model)\n",
    "\n",
    "image = torch.randn(1, 3, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 params: 61.33 M\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"L2 params:\",\n",
    "    round(\n",
    "        summary(l2_sam, image=image, boxes=boxes, device=\"cpu\").total_params / 1000000,\n",
    "        2,\n",
    "    ),\n",
    "    \"M\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 70 time(s)\n",
      "Unsupported operator aten::gelu encountered 58 time(s)\n",
      "Unsupported operator aten::add encountered 76 time(s)\n",
      "Unsupported operator aten::mul encountered 22 time(s)\n",
      "Unsupported operator aten::pad encountered 8 time(s)\n",
      "Unsupported operator aten::div encountered 21 time(s)\n",
      "Unsupported operator aten::upsample_bicubic2d encountered 3 time(s)\n",
      "Unsupported operator aten::mean encountered 4 time(s)\n",
      "Unsupported operator aten::sub encountered 7 time(s)\n",
      "Unsupported operator aten::square encountered 1 time(s)\n",
      "Unsupported operator aten::sqrt encountered 2 time(s)\n",
      "Unsupported operator aten::clone encountered 1 time(s)\n",
      "Unsupported operator aten::sin encountered 2 time(s)\n",
      "Unsupported operator aten::cos encountered 2 time(s)\n",
      "Unsupported operator aten::cumsum encountered 2 time(s)\n",
      "Unsupported operator aten::repeat_interleave encountered 2 time(s)\n",
      "Unsupported operator aten::softmax encountered 7 time(s)\n",
      "Unsupported operator aten::pow encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "model.image_encoder.backbone.stages.0.op_list.1.shortcut, model.image_encoder.backbone.stages.1.op_list.1.shortcut, model.image_encoder.backbone.stages.1.op_list.2.shortcut, model.image_encoder.backbone.stages.2.op_list.1.shortcut, model.image_encoder.backbone.stages.2.op_list.2.shortcut, model.image_encoder.backbone.stages.3.op_list.1.shortcut, model.image_encoder.backbone.stages.3.op_list.2.shortcut, model.image_encoder.backbone.stages.3.op_list.3.shortcut, model.image_encoder.backbone.stages.3.op_list.4.shortcut, model.image_encoder.backbone.stages.3.op_list.5.shortcut, model.image_encoder.backbone.stages.3.op_list.6.shortcut, model.image_encoder.backbone.stages.3.op_list.7.shortcut, model.image_encoder.backbone.stages.3.op_list.8.shortcut, model.image_encoder.backbone.stages.4.op_list.1.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.1.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.2.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.2.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.3.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.3.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.4.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.4.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.5.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.5.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.6.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.6.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.7.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.7.local_module.shortcut, model.image_encoder.backbone.stages.4.op_list.8.context_module.shortcut, model.image_encoder.backbone.stages.4.op_list.8.local_module.shortcut, model.image_encoder.neck.middle.op_list.0.shortcut, model.image_encoder.neck.middle.op_list.1.shortcut, model.image_encoder.neck.middle.op_list.10.shortcut, model.image_encoder.neck.middle.op_list.11.shortcut, model.image_encoder.neck.middle.op_list.2.shortcut, model.image_encoder.neck.middle.op_list.3.shortcut, model.image_encoder.neck.middle.op_list.4.shortcut, model.image_encoder.neck.middle.op_list.5.shortcut, model.image_encoder.neck.middle.op_list.6.shortcut, model.image_encoder.neck.middle.op_list.7.shortcut, model.image_encoder.neck.middle.op_list.8.shortcut, model.image_encoder.neck.middle.op_list.9.shortcut, model.mask_decoder.iou_token, model.mask_decoder.mask_tokens, model.prompt_encoder.mask_downscaling, model.prompt_encoder.mask_downscaling.0, model.prompt_encoder.mask_downscaling.1, model.prompt_encoder.mask_downscaling.2, model.prompt_encoder.mask_downscaling.3, model.prompt_encoder.mask_downscaling.4, model.prompt_encoder.mask_downscaling.5, model.prompt_encoder.mask_downscaling.6, model.prompt_encoder.no_mask_embed, model.prompt_encoder.not_a_point_embed, model.prompt_encoder.point_embeddings.0, model.prompt_encoder.point_embeddings.1, model.prompt_encoder.point_embeddings.2, model.prompt_encoder.point_embeddings.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 flops: 70.71 G\n"
     ]
    }
   ],
   "source": [
    "flops = FlopCountAnalysis(FlopCountWrapper(l2_sam), image)\n",
    "print(\"L2 flops:\", round(flops.total() / 1e9, 2), \"G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedSAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.segment_anything import build_sam_vit_b\n",
    "\n",
    "medsam_model = build_sam_vit_b()\n",
    "medsam_sam = BaseSAM.construct_from(original_sam=medsam_model)\n",
    "\n",
    "image = torch.randn(1, 3, 1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedSAM params: 93.74 M\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"MedSAM params:\",\n",
    "    round(\n",
    "        summary(medsam_sam, image=image, boxes=boxes, device=\"cpu\").total_params\n",
    "        / 1000000,\n",
    "        2,\n",
    "    ),\n",
    "    \"M\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 116 time(s)\n",
      "Unsupported operator aten::rsub encountered 16 time(s)\n",
      "Unsupported operator aten::pad encountered 8 time(s)\n",
      "Unsupported operator aten::mul encountered 167 time(s)\n",
      "Unsupported operator aten::div encountered 86 time(s)\n",
      "Unsupported operator aten::sub encountered 58 time(s)\n",
      "Unsupported operator aten::softmax encountered 19 time(s)\n",
      "Unsupported operator aten::gelu encountered 14 time(s)\n",
      "Unsupported operator aten::mean encountered 6 time(s)\n",
      "Unsupported operator aten::pow encountered 3 time(s)\n",
      "Unsupported operator aten::sqrt encountered 3 time(s)\n",
      "Unsupported operator aten::clone encountered 1 time(s)\n",
      "Unsupported operator aten::sin encountered 2 time(s)\n",
      "Unsupported operator aten::cos encountered 2 time(s)\n",
      "Unsupported operator aten::add_ encountered 2 time(s)\n",
      "Unsupported operator aten::cumsum encountered 2 time(s)\n",
      "Unsupported operator aten::repeat_interleave encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "model.mask_decoder.iou_token, model.mask_decoder.mask_tokens, model.prompt_encoder.mask_downscaling, model.prompt_encoder.mask_downscaling.0, model.prompt_encoder.mask_downscaling.1, model.prompt_encoder.mask_downscaling.2, model.prompt_encoder.mask_downscaling.3, model.prompt_encoder.mask_downscaling.4, model.prompt_encoder.mask_downscaling.5, model.prompt_encoder.mask_downscaling.6, model.prompt_encoder.no_mask_embed, model.prompt_encoder.not_a_point_embed, model.prompt_encoder.point_embeddings.0, model.prompt_encoder.point_embeddings.1, model.prompt_encoder.point_embeddings.2, model.prompt_encoder.point_embeddings.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedSAM flops: 488.24 G\n"
     ]
    }
   ],
   "source": [
    "flops = FlopCountAnalysis(FlopCountWrapper(medsam_sam), image)\n",
    "print(\"MedSAM flops:\", round(flops.total() / 1e9, 2), \"G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteMedSAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.lite_medsam import build_lite_medsam\n",
    "\n",
    "lite_medsam_model = build_lite_medsam()\n",
    "lite_medsam_sam = BaseSAM.construct_from(original_sam=lite_medsam_model)\n",
    "\n",
    "image = torch.randn(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiteMedSAM params: 9.79 M\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"LiteMedSAM params:\",\n",
    "    round(\n",
    "        summary(lite_medsam_sam, image=image, boxes=boxes, device=\"cpu\").total_params\n",
    "        / 1000000,\n",
    "        2,\n",
    "    ),\n",
    "    \"M\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 31 time(s)\n",
      "Unsupported operator aten::gelu encountered 25 time(s)\n",
      "Unsupported operator aten::pad encountered 10 time(s)\n",
      "Unsupported operator aten::mul encountered 45 time(s)\n",
      "Unsupported operator aten::add encountered 57 time(s)\n",
      "Unsupported operator aten::softmax encountered 17 time(s)\n",
      "Unsupported operator aten::mean encountered 6 time(s)\n",
      "Unsupported operator aten::sub encountered 10 time(s)\n",
      "Unsupported operator aten::pow encountered 3 time(s)\n",
      "Unsupported operator aten::sqrt encountered 3 time(s)\n",
      "Unsupported operator aten::div encountered 14 time(s)\n",
      "Unsupported operator aten::clone encountered 1 time(s)\n",
      "Unsupported operator aten::sin encountered 2 time(s)\n",
      "Unsupported operator aten::cos encountered 2 time(s)\n",
      "Unsupported operator aten::cumsum encountered 2 time(s)\n",
      "Unsupported operator aten::repeat_interleave encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "model.mask_decoder.iou_token, model.mask_decoder.mask_tokens, model.prompt_encoder.mask_downscaling, model.prompt_encoder.mask_downscaling.0, model.prompt_encoder.mask_downscaling.1, model.prompt_encoder.mask_downscaling.2, model.prompt_encoder.mask_downscaling.3, model.prompt_encoder.mask_downscaling.4, model.prompt_encoder.mask_downscaling.5, model.prompt_encoder.mask_downscaling.6, model.prompt_encoder.no_mask_embed, model.prompt_encoder.not_a_point_embed, model.prompt_encoder.point_embeddings.0, model.prompt_encoder.point_embeddings.1, model.prompt_encoder.point_embeddings.2, model.prompt_encoder.point_embeddings.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiteMedSAM flops: 39.98 G\n"
     ]
    }
   ],
   "source": [
    "flops = FlopCountAnalysis(FlopCountWrapper(lite_medsam_sam), image)\n",
    "print(\"LiteMedSAM flops:\", round(flops.total() / 1e9, 2), \"G\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efficient-medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
